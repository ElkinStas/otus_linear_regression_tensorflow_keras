{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('Fish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bream</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bream</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bream</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bream</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bream</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4   Bream   430.0     26.5     29.0     34.0  12.4440  5.1340"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### постараемся предсказать вес рыбы по другим параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['Weight'].values\n",
    "x = dataset[['Length1','Length2', 'Length3','Height','Width']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (x - x.mean()) / x.std()\n",
    "y = (y - y.mean()) / y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27., 32., 13., 18.,  6.,  6., 12.,  6., 12.,  4.,  5.,  6.,  6.,\n",
       "         2.,  0.,  1.,  0.,  0.,  1.,  2.]),\n",
       " array([-1.1162267 , -0.88503765, -0.65384861, -0.42265957, -0.19147053,\n",
       "         0.03971852,  0.27090756,  0.5020966 ,  0.73328564,  0.96447468,\n",
       "         1.19566373,  1.42685277,  1.65804181,  1.88923085,  2.1204199 ,\n",
       "         2.35160894,  2.58279798,  2.81398702,  3.04517606,  3.27636511,\n",
       "         3.50755415]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(y, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "Данные нужно разделить в отношении 80/20 на обучающую и валидационную выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = int(len(y) * 0.8)\n",
    "index = np.arange(len(dataset))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "train_index, val_index = index[:pivot], index[pivot:]\n",
    "\n",
    "y_train, y_val = y[train_index], y[val_index]\n",
    "x_train, x_val = x[train_index], x[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация\n",
    "\n",
    "Воспользуйтесь x.shape, чтобы задать правильную размерность вектора с параметрами. Проинициализируйте его случайными значениями.\n",
    "\n",
    "**Значение сдвига** `b = 0`; часто сдвиг или вектор сдвигов инициализируют нулями или средним по выборке значением, если оно отлично от нуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = tf.random.normal(shape=(5,))\n",
    "w = tf.Variable(initial_value, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = tf.random.normal(shape=(5,))\n",
    "w =  tf.Variable(initial_value, dtype=tf.float32)\n",
    "b = tf.Variable(0, dtype=tf.float32)\n",
    "learning_rate = tf.constant(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=\n",
       "array([-0.4603077 ,  0.18801455,  0.2730891 ,  0.18656167,  0.4674958 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, threshold=0.001, n_steps=10000):\n",
    "    x = tf.constant(x, dtype=tf.float32)\n",
    "    y = tf.constant(y, dtype=tf.float32)\n",
    "    for step in range(n_steps):\n",
    "        loss = train_step(x, y)\n",
    "        if loss < threshold:\n",
    "            break\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f\"Loss at {step} iter. is {loss.numpy()}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train step\n",
    "\n",
    "Дополните `train_step` так, чтобы функция вычисляла значение $\\hat{y}$ для переданного $x$, и затем значение функции ошибки с помощью функции `calc_loss`.\n",
    "\n",
    "Добавитье обновление параметров в строчке `w = ...`. *Если возвращаемое значение* функции ошибки равно `nan` или `inf`, подумайте, что могло привести к такой ситуации и что нужно исправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y):\n",
    "    global w\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = calc_loss(y, y_hat)\n",
    "        grads = tape.gradient(loss, w)\n",
    "        #print(grads)\n",
    "    w.assign_sub(grads * 0.001)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножение вектора на вектор или вектора на строку в `tf` - это сложная операция. В отличие от матричного умножения, которое можно записать просто как `C = A @ B` или `C = tf.matmul(A, B)`, здесь нужно вызывать функцию `tf.tensordot`, которая способна умножать тензоры любой размерности. Затем функции надо указать ось, по которой выполняется сложение: в данном случае, 1 - т.е. для произведения матрицы на вектор нужно выполнить операцию\n",
    "\n",
    "`tf.tensordot(x, w, 1)`\n",
    "\n",
    "Воспользовавшись этим способом, напишите модель для линейной регрессии. Кстати, это не единственный возможный вариант [расчета скалярного произведения в tensorflow](https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    global w\n",
    "    return(tf.tensordot(x, w, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, которая вычисляет значение MSE для двух переданных векторов со значениями $y, \\hat{y}$ (истинные и спрогнозированные значения). Функция должна возвращать _одно_ значение (скаляр), как и в формуле $\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2$ \n",
    "\n",
    "* разность векторов и возведение в квадрат можно сделать при помощи обычных операторов `-`, `**2`\n",
    "* Для того, чтобы в тензорфлоу выполнить аналог операции $\\frac{1}{n}\\sum_{i=1}^n (...)$, воспользуйтесь функцией `tf.reduce_mean`. Заодно можете изучить, какие еще есть `tf.reduce_...` функции :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y, y_hat):\n",
    "    x = tf.reduce_mean((y - y_hat)**2)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 49 iter. is 0.6657382845878601\n",
      "Loss at 99 iter. is 0.4110798239707947\n",
      "Loss at 149 iter. is 0.2908097803592682\n",
      "Loss at 199 iter. is 0.2310338169336319\n",
      "Loss at 249 iter. is 0.19914230704307556\n",
      "Loss at 299 iter. is 0.1806071251630783\n",
      "Loss at 349 iter. is 0.16884712874889374\n",
      "Loss at 399 iter. is 0.16079741716384888\n",
      "Loss at 449 iter. is 0.15496587753295898\n",
      "Loss at 499 iter. is 0.15057843923568726\n",
      "Loss at 549 iter. is 0.1471995711326599\n",
      "Loss at 599 iter. is 0.14456145465373993\n",
      "Loss at 649 iter. is 0.14248527586460114\n",
      "Loss at 699 iter. is 0.14084407687187195\n",
      "Loss at 749 iter. is 0.13954336941242218\n",
      "Loss at 799 iter. is 0.13851089775562286\n",
      "Loss at 849 iter. is 0.13769055902957916\n",
      "Loss at 899 iter. is 0.13703832030296326\n",
      "Loss at 949 iter. is 0.13651937246322632\n",
      "Loss at 999 iter. is 0.13610631227493286\n",
      "Loss at 1049 iter. is 0.13577726483345032\n",
      "Loss at 1099 iter. is 0.1355149745941162\n",
      "Loss at 1149 iter. is 0.13530570268630981\n",
      "Loss at 1199 iter. is 0.13513854146003723\n",
      "Loss at 1249 iter. is 0.13500480353832245\n",
      "Loss at 1299 iter. is 0.13489767909049988\n",
      "Loss at 1349 iter. is 0.1348116248846054\n",
      "Loss at 1399 iter. is 0.1347423940896988\n",
      "Loss at 1449 iter. is 0.13468648493289948\n",
      "Loss at 1499 iter. is 0.1346411556005478\n",
      "Loss at 1549 iter. is 0.13460423052310944\n",
      "Loss at 1599 iter. is 0.1345740258693695\n",
      "Loss at 1649 iter. is 0.1345491111278534\n",
      "Loss at 1699 iter. is 0.13452845811843872\n",
      "Loss at 1749 iter. is 0.13451115787029266\n",
      "Loss at 1799 iter. is 0.1344965398311615\n",
      "Loss at 1849 iter. is 0.13448403775691986\n",
      "Loss at 1899 iter. is 0.13447324931621552\n",
      "Loss at 1949 iter. is 0.1344638168811798\n",
      "Loss at 1999 iter. is 0.13445545732975006\n",
      "Loss at 2049 iter. is 0.13444794714450836\n",
      "Loss at 2099 iter. is 0.13444116711616516\n",
      "Loss at 2149 iter. is 0.13443487882614136\n",
      "Loss at 2199 iter. is 0.13442908227443695\n",
      "Loss at 2249 iter. is 0.13442362844944\n",
      "Loss at 2299 iter. is 0.13441845774650574\n",
      "Loss at 2349 iter. is 0.134413480758667\n",
      "Loss at 2399 iter. is 0.13440874218940735\n",
      "Loss at 2449 iter. is 0.13440413773059845\n",
      "Loss at 2499 iter. is 0.1343996822834015\n",
      "Loss at 2549 iter. is 0.1343953013420105\n",
      "Loss at 2599 iter. is 0.13439100980758667\n",
      "Loss at 2649 iter. is 0.1343868225812912\n",
      "Loss at 2699 iter. is 0.1343826800584793\n",
      "Loss at 2749 iter. is 0.1343785971403122\n",
      "Loss at 2799 iter. is 0.13437454402446747\n",
      "Loss at 2849 iter. is 0.1343705654144287\n",
      "Loss at 2899 iter. is 0.13436661660671234\n",
      "Loss at 2949 iter. is 0.13436266779899597\n",
      "Loss at 2999 iter. is 0.13435877859592438\n",
      "Loss at 3049 iter. is 0.13435488939285278\n",
      "Loss at 3099 iter. is 0.13435104489326477\n",
      "Loss at 3149 iter. is 0.13434723019599915\n",
      "Loss at 3199 iter. is 0.13434343039989471\n",
      "Loss at 3249 iter. is 0.13433964550495148\n",
      "Loss at 3299 iter. is 0.13433589041233063\n",
      "Loss at 3349 iter. is 0.13433215022087097\n",
      "Loss at 3399 iter. is 0.1343284249305725\n",
      "Loss at 3449 iter. is 0.13432474434375763\n",
      "Loss at 3499 iter. is 0.13432106375694275\n",
      "Loss at 3549 iter. is 0.13431739807128906\n",
      "Loss at 3599 iter. is 0.13431377708911896\n",
      "Loss at 3649 iter. is 0.13431012630462646\n",
      "Loss at 3699 iter. is 0.13430655002593994\n",
      "Loss at 3749 iter. is 0.1343029886484146\n",
      "Loss at 3799 iter. is 0.13429942727088928\n",
      "Loss at 3849 iter. is 0.13429588079452515\n",
      "Loss at 3899 iter. is 0.1342923492193222\n",
      "Loss at 3949 iter. is 0.13428883254528046\n",
      "Loss at 3999 iter. is 0.1342853456735611\n",
      "Loss at 4049 iter. is 0.13428185880184174\n",
      "Loss at 4099 iter. is 0.13427838683128357\n",
      "Loss at 4149 iter. is 0.1342749446630478\n",
      "Loss at 4199 iter. is 0.1342715173959732\n",
      "Loss at 4249 iter. is 0.134268119931221\n",
      "Loss at 4299 iter. is 0.13426469266414642\n",
      "Loss at 4349 iter. is 0.1342613399028778\n",
      "Loss at 4399 iter. is 0.134257972240448\n",
      "Loss at 4449 iter. is 0.1342546045780182\n",
      "Loss at 4499 iter. is 0.13425125181674957\n",
      "Loss at 4549 iter. is 0.13424797356128693\n",
      "Loss at 4599 iter. is 0.13424468040466309\n",
      "Loss at 4649 iter. is 0.13424138724803925\n",
      "Loss at 4699 iter. is 0.1342381089925766\n",
      "Loss at 4749 iter. is 0.13423484563827515\n",
      "Loss at 4799 iter. is 0.13423162698745728\n",
      "Loss at 4849 iter. is 0.13422836363315582\n",
      "Loss at 4899 iter. is 0.13422518968582153\n",
      "Loss at 4949 iter. is 0.13422200083732605\n",
      "Loss at 4999 iter. is 0.13421882688999176\n",
      "Loss at 5049 iter. is 0.13421565294265747\n",
      "Loss at 5099 iter. is 0.13421250879764557\n",
      "Loss at 5149 iter. is 0.13420936465263367\n",
      "Loss at 5199 iter. is 0.13420623540878296\n",
      "Loss at 5249 iter. is 0.13420315086841583\n",
      "Loss at 5299 iter. is 0.13420003652572632\n",
      "Loss at 5349 iter. is 0.1341969519853592\n",
      "Loss at 5399 iter. is 0.13419389724731445\n",
      "Loss at 5449 iter. is 0.1341908574104309\n",
      "Loss at 5499 iter. is 0.13418780267238617\n",
      "Loss at 5549 iter. is 0.13418476283550262\n",
      "Loss at 5599 iter. is 0.13418175280094147\n",
      "Loss at 5649 iter. is 0.1341787725687027\n",
      "Loss at 5699 iter. is 0.13417579233646393\n",
      "Loss at 5749 iter. is 0.13417284190654755\n",
      "Loss at 5799 iter. is 0.13416986167430878\n",
      "Loss at 5849 iter. is 0.1341669261455536\n",
      "Loss at 5899 iter. is 0.1341640055179596\n",
      "Loss at 5949 iter. is 0.1341610699892044\n",
      "Loss at 5999 iter. is 0.134158194065094\n",
      "Loss at 6049 iter. is 0.1341552734375\n",
      "Loss at 6099 iter. is 0.1341523826122284\n",
      "Loss at 6149 iter. is 0.13414955139160156\n",
      "Loss at 6199 iter. is 0.13414667546749115\n",
      "Loss at 6249 iter. is 0.13414382934570312\n",
      "Loss at 6299 iter. is 0.1341410130262375\n",
      "Loss at 6349 iter. is 0.13413819670677185\n",
      "Loss at 6399 iter. is 0.1341353952884674\n",
      "Loss at 6449 iter. is 0.13413259387016296\n",
      "Loss at 6499 iter. is 0.1341298222541809\n",
      "Loss at 6549 iter. is 0.13412702083587646\n",
      "Loss at 6599 iter. is 0.1341242790222168\n",
      "Loss at 6649 iter. is 0.13412153720855713\n",
      "Loss at 6699 iter. is 0.13411881029605865\n",
      "Loss at 6749 iter. is 0.13411608338356018\n",
      "Loss at 6799 iter. is 0.1341133713722229\n",
      "Loss at 6849 iter. is 0.13411067426204681\n",
      "Loss at 6899 iter. is 0.13410797715187073\n",
      "Loss at 6949 iter. is 0.13410529494285583\n",
      "Loss at 6999 iter. is 0.13410264253616333\n",
      "Loss at 7049 iter. is 0.13409999012947083\n",
      "Loss at 7099 iter. is 0.13409735262393951\n",
      "Loss at 7149 iter. is 0.1340947151184082\n",
      "Loss at 7199 iter. is 0.13409210741519928\n",
      "Loss at 7249 iter. is 0.13408949971199036\n",
      "Loss at 7299 iter. is 0.13408686220645905\n",
      "Loss at 7349 iter. is 0.1340842843055725\n",
      "Loss at 7399 iter. is 0.13408169150352478\n",
      "Loss at 7449 iter. is 0.13407911360263824\n",
      "Loss at 7499 iter. is 0.1340765804052353\n",
      "Loss at 7549 iter. is 0.13407400250434875\n",
      "Loss at 7599 iter. is 0.13407151401042938\n",
      "Loss at 7649 iter. is 0.13406896591186523\n",
      "Loss at 7699 iter. is 0.13406643271446228\n",
      "Loss at 7749 iter. is 0.1340639442205429\n",
      "Loss at 7799 iter. is 0.13406142592430115\n",
      "Loss at 7849 iter. is 0.13405893743038177\n",
      "Loss at 7899 iter. is 0.1340564787387848\n",
      "Loss at 7949 iter. is 0.1340540051460266\n",
      "Loss at 7999 iter. is 0.13405153155326843\n",
      "Loss at 8049 iter. is 0.13404910266399384\n",
      "Loss at 8099 iter. is 0.13404664397239685\n",
      "Loss at 8149 iter. is 0.13404422998428345\n",
      "Loss at 8199 iter. is 0.13404181599617004\n",
      "Loss at 8249 iter. is 0.13403940200805664\n",
      "Loss at 8299 iter. is 0.13403701782226562\n",
      "Loss at 8349 iter. is 0.13403461873531342\n",
      "Loss at 8399 iter. is 0.1340322345495224\n",
      "Loss at 8449 iter. is 0.13402986526489258\n",
      "Loss at 8499 iter. is 0.13402748107910156\n",
      "Loss at 8549 iter. is 0.13402515649795532\n",
      "Loss at 8599 iter. is 0.1340228021144867\n",
      "Loss at 8649 iter. is 0.13402044773101807\n",
      "Loss at 8699 iter. is 0.13401810824871063\n",
      "Loss at 8749 iter. is 0.13401579856872559\n",
      "Loss at 8799 iter. is 0.13401347398757935\n",
      "Loss at 8849 iter. is 0.1340111643075943\n",
      "Loss at 8899 iter. is 0.13400889933109283\n",
      "Loss at 8949 iter. is 0.13400660455226898\n",
      "Loss at 8999 iter. is 0.13400430977344513\n",
      "Loss at 9049 iter. is 0.13400204479694366\n",
      "Loss at 9099 iter. is 0.133999764919281\n",
      "Loss at 9149 iter. is 0.13399754464626312\n",
      "Loss at 9199 iter. is 0.13399529457092285\n",
      "Loss at 9249 iter. is 0.13399305939674377\n",
      "Loss at 9299 iter. is 0.1339908242225647\n",
      "Loss at 9349 iter. is 0.133988618850708\n",
      "Loss at 9399 iter. is 0.13398641347885132\n",
      "Loss at 9449 iter. is 0.13398419320583344\n",
      "Loss at 9499 iter. is 0.13398201763629913\n",
      "Loss at 9549 iter. is 0.13397981226444244\n",
      "Loss at 9599 iter. is 0.13397762179374695\n",
      "Loss at 9649 iter. is 0.13397546112537384\n",
      "Loss at 9699 iter. is 0.13397328555583954\n",
      "Loss at 9749 iter. is 0.13397113978862762\n",
      "Loss at 9799 iter. is 0.13396896421909332\n",
      "Loss at 9849 iter. is 0.1339668184518814\n",
      "Loss at 9899 iter. is 0.13396470248699188\n",
      "Loss at 9949 iter. is 0.13396257162094116\n",
      "Loss at 9999 iter. is 0.13396045565605164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1248173, shape=(), dtype=float32, numpy=0.13396046>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишите функцию для оцени качества модели\n",
    "\n",
    "Функция должна принимать на вход данные и значение целевой переменной возвращать значение ошибки (скаляр). Сравните ошибку на обучающей и валидационной выборке, сделайте выводы о том, переобучена ли модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Буду использовать $R^2$ - коэффициент детерминации. Высчитывается по формуле $1 -\\frac{\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{n}(y^{(i)} - \\bar{y}^{(i)})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y):\n",
    "    x = tf.constant(x, dtype=tf.float32)\n",
    "    y = tf.constant(y, dtype=tf.float32)\n",
    "    y_hat = model(x)\n",
    "    sum2 =(tf.reduce_sum((y - tf.reduce_mean(y))**2))\n",
    "    sum1 = (tf.reduce_sum((y - y_hat)**2))\n",
    "    return(1 - sum1/sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1248225, shape=(), dtype=float32, numpy=0.8794145>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1248254, shape=(), dtype=float32, numpy=0.76172554>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВЫВОДЫ\n",
    "Да, разница в коэффициенте детерминации есть - у обучающей этот показзатель выше. С другой стороный показатель в 0.724 тоже очень хорош и модель может быть использована для прогонозов. Исходя из этого - да, модель преобучена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Слой Linear\n",
    "\n",
    "* Дополните линейный слой так, чтобы у него появились веса для сдвига. Для этого используйте метод `add_weight`. Размерность вектора с весами для сдвига равно размерности слоя на выходе (параметр `units`). \n",
    "* Для инициализации сдвига _нулями_ передайте `tf.keras.initializers.Zeros()` в качестве значения для параметра `add_weight(..., initializer=...)` \n",
    "* Измените метод `call` так, чтобы он использовал сдвиги.\n",
    "* Обучите модель на тех же данных, что и в первом задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, initializer='glorot_uniform', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.initializer = keras.initializers.get(initializer)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "        shape=(input_shape[-1], self.units),\n",
    "        initializer=self.initializer,\n",
    "        name=\"kernel\",\n",
    "        trainable=True,\n",
    "        )\n",
    "        self.baes = self.add_weight(\n",
    "        shape=(input_shape[-1], self.units),\n",
    "        initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"baes\",\n",
    "        trainable=True,\n",
    "        )\n",
    "\n",
    "  \n",
    "    def call(self, inputs):#добавить сдвиг\n",
    "        print()\n",
    "        return tf.matmul(inputs  , self.kernel + self.baes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запустите модель и сравните результаты с моделью из п.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "linear_layer = Linear(1)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                               min_delta=1e-4)\n",
    "model = keras.models.Sequential([linear_layer])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), \n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Обратите внимание**, что добавился параметр `validation_data`, а в `EarlyStopping` в ячейке выше значение параметра `monitor` заменено на `val_loss`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 127 samples, validate on 32 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "Epoch 1/100\n",
      "\n",
      "\n",
      "\n",
      "127/127 [==============================] - 0s 4ms/sample - loss: 1.6550 - val_loss: 1.6572\n",
      "Epoch 2/100\n",
      "127/127 [==============================] - 0s 163us/sample - loss: 1.4797 - val_loss: 1.4688\n",
      "Epoch 3/100\n",
      "127/127 [==============================] - 0s 176us/sample - loss: 1.3318 - val_loss: 1.3087\n",
      "Epoch 4/100\n",
      "127/127 [==============================] - 0s 188us/sample - loss: 1.2054 - val_loss: 1.1717\n",
      "Epoch 5/100\n",
      "127/127 [==============================] - 0s 225us/sample - loss: 1.0961 - val_loss: 1.0536\n",
      "Epoch 6/100\n",
      "127/127 [==============================] - 0s 190us/sample - loss: 1.0005 - val_loss: 0.9512\n",
      "Epoch 7/100\n",
      "127/127 [==============================] - 0s 246us/sample - loss: 0.9164 - val_loss: 0.8617\n",
      "Epoch 8/100\n",
      "127/127 [==============================] - 0s 197us/sample - loss: 0.8417 - val_loss: 0.7833\n",
      "Epoch 9/100\n",
      "127/127 [==============================] - 0s 217us/sample - loss: 0.7750 - val_loss: 0.7141\n",
      "Epoch 10/100\n",
      "127/127 [==============================] - 0s 312us/sample - loss: 0.7153 - val_loss: 0.6530\n",
      "Epoch 11/100\n",
      "127/127 [==============================] - 0s 207us/sample - loss: 0.6616 - val_loss: 0.5987\n",
      "Epoch 12/100\n",
      "127/127 [==============================] - 0s 268us/sample - loss: 0.6132 - val_loss: 0.5504\n",
      "Epoch 13/100\n",
      "127/127 [==============================] - 0s 255us/sample - loss: 0.5694 - val_loss: 0.5072\n",
      "Epoch 14/100\n",
      "127/127 [==============================] - 0s 193us/sample - loss: 0.5298 - val_loss: 0.4686\n",
      "Epoch 15/100\n",
      "127/127 [==============================] - 0s 233us/sample - loss: 0.4940 - val_loss: 0.4340\n",
      "Epoch 16/100\n",
      "127/127 [==============================] - 0s 263us/sample - loss: 0.4614 - val_loss: 0.4029\n",
      "Epoch 17/100\n",
      "127/127 [==============================] - 0s 271us/sample - loss: 0.4318 - val_loss: 0.3749\n",
      "Epoch 18/100\n",
      "127/127 [==============================] - 0s 234us/sample - loss: 0.4050 - val_loss: 0.3497\n",
      "Epoch 19/100\n",
      "127/127 [==============================] - 0s 237us/sample - loss: 0.3806 - val_loss: 0.3270\n",
      "Epoch 20/100\n",
      "127/127 [==============================] - 0s 275us/sample - loss: 0.3584 - val_loss: 0.3065\n",
      "Epoch 21/100\n",
      "127/127 [==============================] - 0s 231us/sample - loss: 0.3382 - val_loss: 0.2880\n",
      "Epoch 22/100\n",
      "127/127 [==============================] - 0s 172us/sample - loss: 0.3199 - val_loss: 0.2712\n",
      "Epoch 23/100\n",
      "127/127 [==============================] - 0s 196us/sample - loss: 0.3032 - val_loss: 0.2560\n",
      "Epoch 24/100\n",
      "127/127 [==============================] - 0s 236us/sample - loss: 0.2880 - val_loss: 0.2423\n",
      "Epoch 25/100\n",
      "127/127 [==============================] - 0s 222us/sample - loss: 0.2742 - val_loss: 0.2298\n",
      "Epoch 26/100\n",
      "127/127 [==============================] - 0s 248us/sample - loss: 0.2616 - val_loss: 0.2185\n",
      "Epoch 27/100\n",
      "127/127 [==============================] - 0s 254us/sample - loss: 0.2502 - val_loss: 0.2083\n",
      "Epoch 28/100\n",
      "127/127 [==============================] - 0s 225us/sample - loss: 0.2398 - val_loss: 0.1990\n",
      "Epoch 29/100\n",
      "127/127 [==============================] - 0s 237us/sample - loss: 0.2303 - val_loss: 0.1906\n",
      "Epoch 30/100\n",
      "127/127 [==============================] - 0s 196us/sample - loss: 0.2217 - val_loss: 0.1829\n",
      "Epoch 31/100\n",
      "127/127 [==============================] - 0s 277us/sample - loss: 0.2138 - val_loss: 0.1760\n",
      "Epoch 32/100\n",
      "127/127 [==============================] - 0s 234us/sample - loss: 0.2067 - val_loss: 0.1697\n",
      "Epoch 33/100\n",
      "127/127 [==============================] - 0s 251us/sample - loss: 0.2002 - val_loss: 0.1639\n",
      "Epoch 34/100\n",
      "127/127 [==============================] - 0s 295us/sample - loss: 0.1943 - val_loss: 0.1587\n",
      "Epoch 35/100\n",
      "127/127 [==============================] - 0s 225us/sample - loss: 0.1889 - val_loss: 0.1540\n",
      "Epoch 36/100\n",
      "127/127 [==============================] - 0s 204us/sample - loss: 0.1840 - val_loss: 0.1497\n",
      "Epoch 37/100\n",
      "127/127 [==============================] - 0s 179us/sample - loss: 0.1795 - val_loss: 0.1458\n",
      "Epoch 38/100\n",
      "127/127 [==============================] - 0s 217us/sample - loss: 0.1754 - val_loss: 0.1422\n",
      "Epoch 39/100\n",
      "127/127 [==============================] - 0s 149us/sample - loss: 0.1717 - val_loss: 0.1390\n",
      "Epoch 40/100\n",
      "127/127 [==============================] - 0s 182us/sample - loss: 0.1684 - val_loss: 0.1360\n",
      "Epoch 41/100\n",
      "127/127 [==============================] - 0s 154us/sample - loss: 0.1653 - val_loss: 0.1334\n",
      "Epoch 42/100\n",
      "127/127 [==============================] - 0s 198us/sample - loss: 0.1625 - val_loss: 0.1309\n",
      "Epoch 43/100\n",
      "127/127 [==============================] - 0s 281us/sample - loss: 0.1600 - val_loss: 0.1287\n",
      "Epoch 44/100\n",
      "127/127 [==============================] - 0s 210us/sample - loss: 0.1576 - val_loss: 0.1267\n",
      "Epoch 45/100\n",
      "127/127 [==============================] - 0s 188us/sample - loss: 0.1555 - val_loss: 0.1249\n",
      "Epoch 46/100\n",
      "127/127 [==============================] - 0s 208us/sample - loss: 0.1536 - val_loss: 0.1232\n",
      "Epoch 47/100\n",
      "127/127 [==============================] - 0s 225us/sample - loss: 0.1519 - val_loss: 0.1217\n",
      "Epoch 48/100\n",
      "127/127 [==============================] - 0s 362us/sample - loss: 0.1503 - val_loss: 0.1203\n",
      "Epoch 49/100\n",
      "127/127 [==============================] - 0s 312us/sample - loss: 0.1488 - val_loss: 0.1191\n",
      "Epoch 50/100\n",
      "127/127 [==============================] - 0s 197us/sample - loss: 0.1475 - val_loss: 0.1179\n",
      "Epoch 51/100\n",
      "127/127 [==============================] - 0s 198us/sample - loss: 0.1463 - val_loss: 0.1169\n",
      "Epoch 52/100\n",
      "127/127 [==============================] - 0s 159us/sample - loss: 0.1452 - val_loss: 0.1159\n",
      "Epoch 53/100\n",
      "127/127 [==============================] - 0s 138us/sample - loss: 0.1442 - val_loss: 0.1151\n",
      "Epoch 54/100\n",
      "127/127 [==============================] - 0s 201us/sample - loss: 0.1433 - val_loss: 0.1143\n",
      "Epoch 55/100\n",
      "127/127 [==============================] - 0s 173us/sample - loss: 0.1425 - val_loss: 0.1136\n",
      "Epoch 56/100\n",
      "127/127 [==============================] - 0s 297us/sample - loss: 0.1417 - val_loss: 0.1130\n",
      "Epoch 57/100\n",
      "127/127 [==============================] - 0s 130us/sample - loss: 0.1410 - val_loss: 0.1124\n",
      "Epoch 58/100\n",
      "127/127 [==============================] - 0s 327us/sample - loss: 0.1404 - val_loss: 0.1118\n",
      "Epoch 59/100\n",
      "127/127 [==============================] - 0s 457us/sample - loss: 0.1398 - val_loss: 0.1113\n",
      "Epoch 60/100\n",
      "127/127 [==============================] - 0s 265us/sample - loss: 0.1393 - val_loss: 0.1109\n",
      "Epoch 61/100\n",
      "127/127 [==============================] - 0s 401us/sample - loss: 0.1388 - val_loss: 0.1105\n",
      "Epoch 62/100\n",
      "127/127 [==============================] - 0s 322us/sample - loss: 0.1384 - val_loss: 0.1101\n",
      "Epoch 63/100\n",
      "127/127 [==============================] - 0s 283us/sample - loss: 0.1380 - val_loss: 0.1098\n",
      "Epoch 64/100\n",
      "127/127 [==============================] - 0s 266us/sample - loss: 0.1376 - val_loss: 0.1095\n",
      "Epoch 65/100\n",
      "127/127 [==============================] - 0s 274us/sample - loss: 0.1373 - val_loss: 0.1092\n",
      "Epoch 66/100\n",
      "127/127 [==============================] - 0s 161us/sample - loss: 0.1370 - val_loss: 0.1090\n",
      "Epoch 67/100\n",
      "127/127 [==============================] - 0s 296us/sample - loss: 0.1367 - val_loss: 0.1087\n",
      "Epoch 68/100\n",
      "127/127 [==============================] - 0s 215us/sample - loss: 0.1365 - val_loss: 0.1085\n",
      "Epoch 69/100\n",
      "127/127 [==============================] - 0s 285us/sample - loss: 0.1363 - val_loss: 0.1083\n",
      "Epoch 70/100\n",
      "127/127 [==============================] - 0s 342us/sample - loss: 0.1361 - val_loss: 0.1081\n",
      "Epoch 71/100\n",
      "127/127 [==============================] - 0s 409us/sample - loss: 0.1359 - val_loss: 0.1080\n",
      "Epoch 72/100\n",
      "127/127 [==============================] - 0s 220us/sample - loss: 0.1357 - val_loss: 0.1078\n",
      "Epoch 73/100\n",
      "127/127 [==============================] - 0s 338us/sample - loss: 0.1355 - val_loss: 0.1077\n",
      "Epoch 74/100\n",
      "127/127 [==============================] - 0s 168us/sample - loss: 0.1354 - val_loss: 0.1076\n",
      "Epoch 75/100\n",
      "127/127 [==============================] - 0s 204us/sample - loss: 0.1352 - val_loss: 0.1075\n",
      "Epoch 76/100\n",
      "127/127 [==============================] - 0s 235us/sample - loss: 0.1351 - val_loss: 0.1074\n",
      "Epoch 77/100\n",
      "127/127 [==============================] - 0s 215us/sample - loss: 0.1350 - val_loss: 0.1073\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, np.expand_dims(y_train, axis=-1), \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=512,\n",
    "                    epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "> Выведите полученные результаты в tensorboard. Это сервис, который позволяет визуализировать процесс обучения модели, состояние графа и не только. \n",
    "\n",
    "Для запуска нужно \n",
    "* Установить расширение\n",
    "* Выполнить команду запуска, передав ей имя директории, в которую записывал логи `callbacks.Tensorboard`.\n",
    "\n",
    "Более подробно изучим tensorboard на будущих занятиях. Пока можете посмотреть вкладки scalars, graphs, distributions, histograms. В них содержится информация о метриках модели, графе, распередлениях и гистограммах параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a03e336622ed5266\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a03e336622ed5266\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir='logs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 3019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
